{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7531185,"sourceType":"datasetVersion","datasetId":4386420},{"sourceId":7531276,"sourceType":"datasetVersion","datasetId":4386472},{"sourceId":7532000,"sourceType":"datasetVersion","datasetId":4386883},{"sourceId":7596518,"sourceType":"datasetVersion","datasetId":4421793},{"sourceId":7596565,"sourceType":"datasetVersion","datasetId":4421811}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing stock ml libraries\nimport warnings\nwarnings.simplefilter('ignore')\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport logging\nlogging.basicConfig(level=logging.ERROR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing stock ml libraries\nimport warnings\nwarnings.simplefilter('ignore')\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import RobertaTokenizer, RobertaModel\nimport logging\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nlogging.basicConfig(level=logging.ERROR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchmetrics import F1Score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Setting up the device for GPU usage\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sections of config\n\n# Defining some key variables that will be used later on in the training\nMAX_LEN = 500\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 4\nEPOCHS = 8\nLEARNING_RATE = 1e-05\n# Load the DistilBERT tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiLabelDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text\n        self.targets = self.data.labels\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import DistilBertModel, DistilBertTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DistilBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(DistilBERTClass, self).__init__()\n        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.1)\n        self.additional_fc1 = torch.nn.Linear(768, 1024)  \n        self.additional_fc2 = torch.nn.Linear(1024, 512)  \n        self.classifier = torch.nn.Linear(512, 57)        \n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.Tanh()(pooler)\n        pooler = self.dropout(pooler)\n        pooler = F.relu(self.additional_fc1(pooler))\n        pooler = F.leaky_relu(self.additional_fc2(pooler))\n\n        output = self.classifier(pooler)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DistilBERTClass()\nmodel.load_state_dict(torch.load('/kaggle/input/distilbert/bert_model_trained_epoch_199.pt'))\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combined Code Block\n#Formatting the test data\nimport numpy as np\nimport pandas as pd\ntest_df = pd.read_csv('/kaggle/input/test-data1/test.csv')###INSERT TEST DATA HERE###\ntest_processed = pd.DataFrame()\ntest_processed[\"Id\"] = test_df['Id']\ntest_df['Title'] = test_df['Title'].str.lower()\ntest_df['Abstract'] = test_df['Abstract'].str.lower()\ntest_processed['Text'] = \" \" + test_df['Title'] + '\\n' + test_df['Abstract']\narticles = ['a', 'an', 'the', 'this', 'by', 'that', 'there', 'of', 'on', 'in', 'or', 'and', 'at', 'but', 'therefore', 'henceforth', 'to', 'aims', 'discusses', 'presents', 'considers', 'analyzes', 'explains', 'covers', 'deals', 'with', 'about', 'plays', 'has', 'much', 'been', 'attention', 'have', 'shown', 'several', 'efforts', 'for', 'to', 'it', ',', '.', '!', '', '$', '%', '#', '@']\ndef remove_words(text):\n    for word in articles:\n        text = text.replace(f' {word} ', ' ')\n    return text.strip()\ntest_processed['Text'] = test_processed['Text'].apply(remove_words)\ncommon_stop_words = [\n    'copyright', 'study', 'researcher', 'materials', 'proceedings', 'university', 'case', 'fax', 'keywords', 'discussed',\n    'validates', 'has', 'there', 'dealing', 'has', 'well', 'additionally', 'volume', 'that', 'new', 'several', 'been', 'also',\n    'however', 'attention', 'discussion', 'doi', 'validates', 'conclusion', 'address', 'published', 'has', 'several', 'therefore',\n    'et', 'evaluating', 'determines', 'novelty', 'his', 'theirs', 'deals', 'solve', 'could', 'shown', 'considers', 'attention',\n    'our', 'been', 'discussion', 'being', 'volume', 'concluding', 'aims', 'shows', 'journal', 'thus', 'study', 'it', 'much', 'solving',\n    'towards', 'myself', 'have', 'reproduce', 'analyzing', 'at', 'discussing', 'propose', 'but', 'considering', 'documents', 'about',\n    'issue', 'analysed', 'to', 'investigation', 'will', 'be', 'shown', 'several', 'evaluation', 'address', 'validation', 'but', 'several',\n    'ours', 'hers', 'solves', 'introduction', 'which', 'use', 'novel', 'illustrates', 'shown', 'much', 'novel', 'document', 'addresses',\n    'solve', 'solves', 'show', 'examining', 'experiments', 'you', 'concluding', 'has', 'would', 'been', 'evaluation', 'experiments', 'method',\n    'showing', 'proposing', 'been', 'have', 'investigation', 'we', 'uses', 'has', 'been', 'there', 'experimental', 'with', 'much', 'studies', 'shown',\n    'discussion', 'it', 'novelty', 'a', 'new', 'henceforth', 'investigation', 'showing', 'aim', 'results', 'shown', 'myself', 'there', 'discusses',\n    'analyze', 'it', 'him', 'ourselves', 'proposing', 'dealing', 'much', 'much', 'result', 'experiment', 'there', 'been', 'reproduce', 'novelty', 'also',\n    'much', 'analyzing', 'shown', 'shown', 'our', 'determine', 'that', 'conclusion', 'herself', 'documented', 'been', 'technique', 'examining', 'much',\n    'investigations', 'much', 'novel', 'comparisons', 'survey', 'much', 'is', 'shown', 'uses', 'therefore', 'discusses', 'the', 'much', 'original', 'that',\n    'shown', 'much', 'shown', 'experimental', 'much', 'comparisons', 'case', 'been', 'shown', 'comparisons', 'we', 'much', 'result', 'shows', 'dealing', 'shown',\n    'illustrate', 'techniques', 'deals', 'conclusion', 'corresponding', 'in', 'comparisons', 'much', 'also', 'shown', 'there', 'determine', 'my', 'there', 'shown',\n    'conclusively', 'comparisons', 'much', 'concluding', 'comparisons', 'we', 'validating', 'proposes', 'been', 'address', 'shown', 'deals', 'there', 'comparisons',\n    'much', 'shown', 'been', 'techniques', 'ourselves', 'shown', 'much', 'concluding', 'been', 'technique', 'myself', 'there', 'experimental', 'be', 'examining', 'be',\n    'been', 'case', 'a', 'much', 'discusses', 'been', 'shown', 'proposing', 'we', 'been', 'discusses', 'been', 'shown', 'been', 'technique', 'also', 'much', 'there', 'been',\n    'we', 'addresses', 'we', 'been', 'there', 'been', 'my', 'validates', 'been', 'there', 'shown', 'been', 'shown', 'there', 'much', 'been', 'much', 'been', 'addresses', 'has', 'been',\n    'we', 'proposing', 'been', 'there', 'been', 'there', 'been', 'techniques', 'been', 'techniques', 'techniques', 'shown', 'comparisons', 'be', 'been', 'novel', 'been', 'shown', 'been',\n    'validates', 'been', 'novel', 'shown', 'addresses', 'addresses', 'proposing', 'been', 'technique', 'shown', 'been', 'techniques', 'addresses', 'addresses', 'addresses', 'there', 'techniques',\n    'been', 'techniques', 'abstract', 'article', 'paper', 'title', 'author', 'authors', 'keywords', 'summary', 'introduction',\n    'conclusion', 'method', 'results', 'discussion', 'proceedings', 'journal', 'volume', 'issue', 'doi', 'published',\n    'published', 'conference', 'university', 'copyright', 'rights', 'reserved', 'email', 'corresponding', 'address',\n    'tel', 'fax', 'et', 'al', 'figure', 'figures', 'table', 'tables', 'figure', 'figures', 'table', 'tables',\n    'data', 'methodology', 'methodologies', 'study', 'studies', 'experiment', 'experiments', 'research', 'result',\n    'results', 'analysis', 'discussion', 'discussion', 'discussions', 'method', 'methods', 'materials', 'material',\n    'abstract', 'article', 'paper', 'title', 'author', 'authors', 'keywords', 'summary', 'introduction',\n    'conclusion', 'method', 'results', 'discussion', 'proceedings', 'journal', 'volume', 'issue', 'doi', 'published',\n    'published', 'conference', 'university', 'copyright', 'rights', 'reserved', 'email', 'corresponding', 'address',\n    'tel', 'fax', 'et', 'al', 'figure', 'figures', 'table', 'tables', 'figure', 'figures', 'table', 'tables',\n    'data', 'methodology', 'methodologies', 'study', 'studies', 'experiment', 'experiments', 'research', 'result',\n    'results', 'analysis', 'discussion', 'discussion', 'discussions', 'method', 'methods', 'materials', 'material',\n    'conclusion', 'conclusions', 'conclude', 'concluding', 'conclusively', 'discussion', 'discussions', 'discuss',\n    'discussed', 'discussing', 'approach', 'approaches', 'used', 'using', 'use', 'based', 'case', 'study', 'study',\n    'studies', 'investigate', 'investigation', 'evaluate', 'evaluation', 'assess', 'assessment', 'analyze', 'analysis',\n    'propose', 'proposed', 'method', 'methods', 'technique', 'techniques', 'novel', 'novelty', 'new', 'original', 'work',\n    'researcher', 'researchers', 'work', 'works', 'demonstrate', 'demonstrates', 'demonstrated', 'experimental', 'study',\n    'studies', 'survey', 'surveys', 'experiment', 'experiments', 'experimental', 'validation', 'validate', 'validates',\n    'validating', 'simulation', 'simulations', 'simulation', 'result', 'results', 'model', 'models', 'modeling', 'modelled',\n    'model', 'analysis', 'analyses', 'analyze', 'analysed', 'analysing', 'method', 'methods', 'methodology', 'compare', 'compares',\n    'comparing', 'comparison', 'comparisons', 'evaluation', 'evaluate', 'evaluates', 'evaluating', 'conclusion', 'conclusions',\n    'conclude', 'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents', 'documented', 'documenting', 'report',\n    'reports', 'reported', 'reporting', 'investigation', 'investigations', 'study', 'studies', 'case', 'cases', 'study', 'studies',\n    'show', 'shows', 'showing', 'illustrate', 'illustrates', 'illustrating', 'present', 'presents', 'presenting', 'determine',\n    'determines', 'determining', 'discuss', 'discusses', 'discussing', 'propose', 'proposes', 'proposing', 'address', 'addresses',\n    'addressing', 'solve', 'solves', 'solving', 'solved', 'approach', 'approaches', 'novel', 'novelty', 'new', 'original', 'use',\n    'using', 'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods', 'technique', 'techniques', 'experimental', 'experiment',\n    'experiments', 'survey', 'surveys', 'simulation', 'simulations', 'model', 'models', 'modeling', 'validation', 'validate',\n    'validates', 'validating', 'comparison', 'comparisons', 'evaluation', 'evaluate', 'evaluates', 'evaluating', 'conclusion',\n    'conclusions', 'conclude', 'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents', 'report', 'reports',\n    'investigation', 'investigations', 'study', 'studies', 'case', 'cases', 'show', 'shows', 'showing', 'illustrate', 'illustrates',\n    'illustrating', 'present', 'presents', 'presenting', 'determine', 'determines', 'determining', 'discuss', 'discusses', 'discussing',\n    'propose', 'proposes', 'proposing', 'address', 'addresses', 'addressing', 'solve', 'solves', 'solving', 'solved', 'approach',\n    'approaches', 'novel', 'novelty', 'new', 'original', 'use', 'using', 'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods',\n    'technique', 'techniques', 'experimental', 'experiment', 'experiments', 'survey', 'surveys', 'simulation', 'simulations', 'model',\n    'models', 'modeling', 'validation', 'validate', 'validates', 'validating', 'comparison', 'comparisons', 'evaluation', 'evaluate',\n    'evaluates', 'evaluating', 'conclusion', 'conclusions', 'conclude', 'concluding', 'conclusively', 'paper', 'papers', 'document',\n    'documents', 'report', 'reports', 'investigation', 'investigations', 'study', 'studies', 'case', 'cases', 'show', 'shows', 'showing',\n    'illustrate', 'illustrates', 'illustrating', 'present', 'presents', 'presenting', 'determine', 'determines', 'determining', 'discuss',\n    'discusses', 'discussing', 'propose', 'proposes', 'proposing', 'address', 'addresses', 'addressing', 'solve', 'solves', 'solving', 'solved',\n    'approach', 'approaches', 'novel', 'novelty', 'new', 'original', 'use', 'using', 'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods',\n    'technique', 'techniques', 'experimental', 'experiment', 'experiments', 'survey', 'surveys', 'simulation', 'simulations', 'model', 'models',\n    'modeling', 'validation', 'validate', 'validates', 'validating', 'comparison', 'comparisons', 'evaluation', 'evaluate', 'evaluates', 'evaluating',\n    'conclusion', 'conclusions', 'conclude', 'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents', 'report', 'reports', 'investigation',\n    'investigations', 'study', 'studies', 'case', 'cases', 'show', 'shows', 'showing', 'illustrate', 'illustrates', 'illustrating', 'present', 'presents',\n    'presenting', 'determine', 'determines', 'determining', 'discuss', 'discusses', 'discussing', 'propose', 'proposes', 'proposing', 'address', 'addresses',\n    'addressing', 'solve', 'solves', 'solving', 'solved', 'approach', 'approaches', 'novel', 'novelty', 'new', 'original', 'use', 'using', 'used', 'utilize',\n    'utilizes', 'utilizing', 'method', 'methods', 'technique', 'techniques', 'experimental', 'experiment', 'experiments', 'survey', 'surveys', 'simulation',\n    'simulations', 'model', 'models', 'modeling', 'validation', 'validate', 'validates', 'validating', 'comparison', 'comparisons', 'evaluation', 'evaluate',\n    'evaluates', 'evaluating', 'conclusion', 'conclusions', 'conclude', 'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents', 'report',\n    'reports', 'investigation', 'investigations', 'study', 'studies', 'case', 'cases', 'show', 'shows', 'showing', 'illustrate', 'illustrates', 'illustrating',\n    'present', 'presents', 'presenting', 'determine', 'determines', 'determining', 'discuss', 'discusses', 'discussing', 'propose', 'proposes', 'proposing',\n    'address', 'addresses', 'addressing', 'solve', 'solves', 'solving', 'solved', 'approach', 'approaches', 'novel', 'novelty', 'new', 'original', 'use', 'using',\n    'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods', 'technique', 'techniques', 'experimental', 'experiment', 'experiments', 'survey', 'surveys',\n    'simulation', 'simulations', 'model', 'models', 'modeling', 'validation', 'validate', 'validates', 'validating', 'comparison', 'comparisons', 'evaluation',\n    'evaluate', 'evaluates', 'evaluating', 'conclusion', 'conclusions', 'conclude', 'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents',\n    'report', 'reports', 'investigation', 'investigations', 'study', 'studies', 'case', 'cases', 'show', 'shows', 'showing', 'illustrate', 'illustrates',\n    'illustrating', 'present', 'presents', 'presenting', 'determine', 'determines', 'determining', 'discuss', 'discusses', 'discussing', 'propose', 'proposes',\n    'proposing', 'address', 'addresses', 'addressing', 'solve', 'solves', 'solving', 'solved', 'approach', 'approaches', 'novel', 'novelty', 'new', 'original',\n    'use', 'using', 'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods', 'technique', 'techniques', 'experimental', 'experiment', 'experiments',\n    'survey', 'surveys', 'simulation', 'simulations', 'model', 'models', 'modeling', 'validation', 'validate', 'validates', 'validating', 'comparison',\n    'comparisons', 'evaluation', 'evaluate', 'evaluates', 'evaluating', 'conclusion', 'conclusions', 'conclude', 'concluding', 'conclusively', 'paper',\n    'papers', 'document', 'documents', 'report', 'reports', 'investigation', 'investigations', 'study', 'studies', 'case', 'cases', 'show', 'shows',\n    'showing', 'illustrate', 'illustrates', 'illustrating', 'present', 'presents', 'presenting', 'determine', 'determines', 'determining', 'discuss',\n    'discusses', 'discussing', 'propose', 'proposes', 'proposing', 'address', 'addresses', 'addressing', 'solve', 'solves', 'solving', 'solved', 'approach',\n    'approaches', 'novel', 'novelty', 'new', 'original', 'use', 'using', 'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods', 'technique', 'techniques',\n    'experimental', 'experiment', 'experiments', 'survey', 'surveys', 'simulation', 'simulations', 'model', 'models', 'modeling', 'validation', 'validate',\n    'validates', 'validating', 'comparison', 'comparisons', 'evaluation', 'evaluate', 'evaluates', 'evaluating', 'conclusion', 'conclusions', 'conclude',\n    'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents', 'report', 'reports', 'investigation', 'investigations', 'study', 'studies', 'case',\n    'cases', 'show', 'shows', 'showing', 'illustrate', 'illustrates', 'illustrating', 'present', 'presents', 'presenting', 'determine', 'determines', 'determining',\n    'discuss', 'discusses', 'discussing', 'propose', 'proposes', 'proposing', 'address', 'addresses', 'addressing', 'solve', 'solves', 'solving', 'solved', 'approach',\n    'approaches', 'novel', 'novelty', 'new', 'original', 'use', 'using', 'used', 'utilize', 'utilizes', 'utilizing', 'method', 'methods', 'technique', 'techniques',\n    'experimental', 'experiment', 'experiments', 'survey', 'surveys', 'simulation', 'simulations', 'model', 'models', 'modeling', 'validation', 'validate',\n    'validates', 'validating', 'comparison', 'comparisons', 'evaluation', 'evaluate', 'evaluates', 'evaluating', 'conclusion', 'conclusions', 'conclude',\n    'concluding', 'conclusively', 'paper', 'papers', 'document', 'documents', 'report', 'reports', 'investigation', 'investigations', 'study', 'studies', 'case',\n    'cases', 'show', 'shows', 'showing', 'illustrate', 'illustrates', 'illustrating', 'present', 'presents', 'presenting', 'determine', 'determines', 'determining',\n    'discuss', 'discusses', 'discussing', 'propose']\ncommon_stop_words = list(set(common_stop_words))\ndef remove_words(text):\n    for word in common_stop_words:\n        text = text.replace(f' {word} ', ' ')\n    return text.strip()\ntest_processed['Text'] = test_processed['Text'].apply(remove_words)\nprint(test_processed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntest_loader = DataLoader(test_processed, batch_size=VALID_BATCH_SIZE, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\n\ndef predict(model, dataloader):\n    model.eval()\n    all_predictions = []\n    #all_indices = []\n\n    with torch.no_grad():\n        for batch_index, data in tqdm(enumerate(dataloader)):\n            ids = data['ids'].to(device, dtype=torch.long).int()\n            mask = data['mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n\n            outputs = model(ids, mask, token_type_ids)\n\n            predictions = outputs\n            all_predictions.append(predictions.cpu().numpy())\n\n    all_predictions = np.vstack(all_predictions)\n    return all_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_final = pd.DataFrame()\ntest_processed\ntest_final['text'] = test_processed['Text']\ntest_final","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PredictLabel1(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_data = test_final#.reset_index(drop=True)\n\ntesting_set = PredictLabel1(testing_data, tokenizer, MAX_LEN)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unlabelled_predictions = predict(model, testing_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sigm = unlabelled_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sigm = 1 / (1 + np.exp(-np.array(sigm)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final1 = sigm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11 = final1 >= 0.608","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11 = final11 *1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11 = pd.DataFrame(final11)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = pd.read_csv('/kaggle/input/data-columns/Processed_Data.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11.columns = columns.columns[4:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11['Id'] = test_processed['Id']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_cols = pd.read_csv('/kaggle/input/sample-columns/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11= final11[sample_cols.columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11\n#This is DistilBERT output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 350\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nEPOCHS = 25\nLEARNING_RATE = 1e-05\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiLabelDataset1(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text\n        self.targets = self.data.labels\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,\n            \n            return_tensors = 'pt'\n        )\n        ids = inputs['input_ids'].squeeze()\n        mask = inputs['attention_mask'].squeeze()\n        token_type_ids = inputs[\"token_type_ids\"].squeeze()\n\n\n        return {\n            'ids': ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids,\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaClass(torch.nn.Module):\n    def __init__(self):\n        super(RoBERTaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n        self.linear1 = torch.nn.Linear(768, 512)\n        self.dropout = torch.nn.Dropout(0.1)\n        self.linear2 = torch.nn.Linear(512, 256)\n        self.leaky_relu = torch.nn.LeakyReLU()\n        self.linear3 = torch.nn.Linear(256, 64)\n        self.tanh = torch.nn.Tanh()\n        self.classifier = torch.nn.Linear(64, 57)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1.last_hidden_state\n        pooler = hidden_state[:, 0]\n        linear1_output = self.linear1(pooler)\n        linear1_output = self.dropout(linear1_output)\n        linear2_output = self.linear2(linear1_output)\n        linear2_output = self.leaky_relu(linear2_output)\n        linear3_output = self.linear3(linear2_output)\n        linear3_output = self.leaky_relu(linear3_output)\n        output = self.classifier(linear3_output)\n        return output\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = RoBERTaClass()\nmodel2.load_state_dict(torch.load('/kaggle/input/roberta/roberta_model_epoch_25.pt'))\nmodel2 = model2.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntest_df = pd.read_csv('/kaggle/input/test-data1/test.csv')###INSERT TEST DATA HERE###\ntest_processed = pd.DataFrame()\ntest_processed[\"Id\"] = test_df['Id']\ntest_df['Title'] = test_df['Title'].str.lower()\ntest_df['Abstract'] = test_df['Abstract'].str.lower()\ntest_processed['Text'] =test_df['Title'] + '. ' + test_df['Abstract']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Assuming you have a dataset for your unlabelled test data (test_dataset)\ntest_loader = DataLoader(test_processed, batch_size=VALID_BATCH_SIZE, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\n\ndef predict1(model, dataloader):\n    model2.eval()\n    all_predictions = []\n    #all_indices = []\n\n    with torch.no_grad():\n        for batch_index, data in tqdm(enumerate(dataloader)):\n            ids = data['ids'].to(device, dtype=torch.long).int()\n            mask = data['mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n\n            outputs = model2(ids, mask, token_type_ids)\n\n            predictions = outputs  \n            all_predictions.append(predictions.cpu().numpy())\n\n    all_predictions = np.vstack(all_predictions)\n    return all_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PredictLabel(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_final = pd.DataFrame()\ntest_final['text'] = test_processed['Text']\ntesting_data = test_final\ntesting_set = PredictLabel(testing_data, tokenizer, MAX_LEN)\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unlabelled_predictions = predict1(model2, testing_loader)\nsigm = unlabelled_predictions\nsigm = 1 / (1 + np.exp(-np.array(sigm)))\nsi = sigm\nsi = pd.DataFrame(si)\nfinal11_2 = si","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yetp= sigm\nsave = sigm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11_2 = final11_2 >= 0.5694\nfinal11_2 = final11_2 *1\ncolumns = pd.read_csv('/kaggle/input/data-columns/Processed_Data.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11_2.columns = columns.columns[4:]\nfinal11['Id'] = test_processed['Id']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_cols = pd.read_csv('/kaggle/input/sample-columns/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final11_u = final11 + final11_2\nfinal11_u = final11_u >= 1\nfinal11_u = final11_u * 1\nfinal11_u['Id'] =  test_processed['Id']\nfinal11_u = final11_u[sample_cols.columns]\nfinal11_u.to_csv('barak.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]}]}